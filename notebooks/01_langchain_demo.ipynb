{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3d054b",
   "metadata": {},
   "source": [
    "#### LangChain Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcbbc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Hello Rushi \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3d3aa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyBTLN3Nree6fhKxa0RNiLZtvyrY-EFkB-8\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b03f9",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "83febe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7494c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sync client is not available. This happens when an async callable was provided for the API key. Use async methods (ainvoke, astream) instead, or provide a string or sync callable for the API key.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m llm = ChatOpenAI(\n\u001b[32m      2\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mopenai/gpt-oss-120b:free\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     base_url=\u001b[33m\"\u001b[39m\u001b[33mhttps://openrouter.ai/api/v1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENROUTER_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m res = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWho is the President of the India ?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.content\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(res.replace(\u001b[33m\"\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CodingPlayground\\GenAI\\UnWrapGenAI\\ProjectSetUp\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CodingPlayground\\GenAI\\UnWrapGenAI\\ProjectSetUp\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CodingPlayground\\GenAI\\UnWrapGenAI\\ProjectSetUp\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CodingPlayground\\GenAI\\UnWrapGenAI\\ProjectSetUp\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CodingPlayground\\GenAI\\UnWrapGenAI\\ProjectSetUp\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1345\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1338\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1339\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1340\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1343\u001b[39m     **kwargs: Any,\n\u001b[32m   1344\u001b[39m ) -> ChatResult:\n\u001b[32m-> \u001b[39m\u001b[32m1345\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_sync_client_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m     payload = \u001b[38;5;28mself\u001b[39m._get_request_payload(messages, stop=stop, **kwargs)\n\u001b[32m   1347\u001b[39m     generation_info = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CodingPlayground\\GenAI\\UnWrapGenAI\\ProjectSetUp\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1137\u001b[39m, in \u001b[36mBaseChatOpenAI._ensure_sync_client_available\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1132\u001b[39m     msg = (\n\u001b[32m   1133\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSync client is not available. This happens when an async callable \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1134\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwas provided for the API key. Use async methods (ainvoke, astream) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1135\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minstead, or provide a string or sync callable for the API key.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1136\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Sync client is not available. This happens when an async callable was provided for the API key. Use async methods (ainvoke, astream) instead, or provide a string or sync callable for the API key."
     ]
    }
   ],
   "source": [
    "# llm = ChatOpenAI(\n",
    "#     model=\"openai/gpt-oss-120b:free\",\n",
    "#     base_url=\"https://openrouter.ai/api/v1\",\n",
    "#     api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    "# )\n",
    "\n",
    "# res = llm.invoke(\"Who is the President of the India ?\").content\n",
    "\n",
    "# print(res.replace(\". \", \".\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65c9fff",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2e85688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a007e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='GenAI is a type of artificial intelligence that can create new content, such as text, images, and music, based on the data it has been trained on.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019ba5f6-29a2-7091-bad4-52273b8af839-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 8, 'output_tokens': 33, 'total_tokens': 41, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = llm.invoke(\"Explain GenAI in one sentence.\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac97d154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenAI is a type of artificial intelligence that can create new content, such as text, images, and music, based on the data it has been trained on.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805340a",
   "metadata": {},
   "source": [
    "#### Static Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d857f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7160f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c003b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ah, the age-old question! Deploying on a Friday is a classic DevOps taboo, and for good reason. While there are always exceptions and sometimes you might have to, the general wisdom is to **avoid deploying on Fridays, especially late on a Friday.**\\n\\nHere\\'s a breakdown of why it\\'s generally a bad idea:\\n\\n**1. The Weekend is Your Enemy (for Rollbacks):**\\n\\n*   **Limited Support Staff:** Most of your critical engineering, operations, and support teams are likely winding down for the weekend. If something goes wrong, getting the right people on a call, making quick decisions, and executing a rollback can be significantly harder and slower.\\n*   **Reduced Availability:** Key personnel might be out of town, on vacation, or simply unreachable, making incident response a nightmare.\\n*   **Escalation Challenges:** If a critical issue arises, escalating it to the right people on a Saturday or Sunday can be a real struggle, leading to prolonged downtime and frustrated users.\\n\\n**2. Increased Risk of Unforeseen Issues:**\\n\\n*   **\"It Worked on My Machine\" Syndrome:** Friday deployments are often rushed. Developers might be eager to finish up for the week, leading to less thorough testing or overlooking edge cases.\\n*   **Intermittent Bugs:** Some bugs only manifest under specific load conditions or at certain times. A Friday deployment might seem fine initially, but problems could surface over the weekend when fewer people are monitoring.\\n*   **External Dependencies:** If your deployment relies on external services or third-party APIs, their behavior over the weekend might be different or they might have their own maintenance windows, leading to unexpected failures.\\n\\n**3. Impact on User Experience:**\\n\\n*   **Downtime is More Painful:** If your application experiences downtime or critical bugs over the weekend, a larger percentage of your user base might be affected. People have more free time to use your services, and a broken experience can lead to significant dissatisfaction.\\n*   **Reputational Damage:** A major outage over a weekend can be highly visible and negatively impact your company\\'s reputation.\\n\\n**4. Reduced Morale and Work-Life Balance:**\\n\\n*   **The Dreaded Weekend Call:** Nobody wants to spend their weekend firefighting a production issue caused by a deployment. This can lead to burnout and resentment among the team.\\n*   **Unplanned Work:** A failed Friday deployment often means the entire team (or at least the on-call folks) is stuck working through the weekend, ruining their plans.\\n\\n**5. Difficulty in Getting Feedback and Iteration:**\\n\\n*   **Slower Feedback Loop:** If you need to gather feedback on the new release or make quick adjustments, the weekend significantly slows down this process.\\n*   **Missed Opportunities:** You might miss out on valuable user feedback or the opportunity to iterate on a new feature quickly because the team is unavailable.\\n\\n**When might you *consider* a Friday deploy (with extreme caution)?**\\n\\n*   **Trivial, Low-Risk Updates:** If it\\'s a very minor bug fix, a small configuration change, or a feature with a very limited blast radius and a robust rollback plan.\\n*   **Urgent Security Patches:** In critical situations where a vulnerability needs immediate addressing, a Friday deploy might be unavoidable, but it should be accompanied by maximum vigilance and preparedness.\\n*   **Pre-Planned Maintenance with Communication:** If you have a scheduled maintenance window that happens to fall on a Friday, and you\\'ve communicated this extensively to your users, it might be acceptable.\\n*   **Highly Automated and Resilient Systems:** If your deployment pipeline is incredibly robust, your monitoring is top-notch, and your rollback strategy is nearly instantaneous, the risk might be lower.\\n\\n**The Golden Rule:**\\n\\nThe best practice is to deploy earlier in the week (Monday, Tuesday, or Wednesday) to give yourself ample time to:\\n\\n*   Monitor the deployment.\\n*   Address any issues that arise.\\n*   Gather feedback.\\n*   Perform rollbacks if necessary.\\n*   Enjoy your weekend!\\n\\nSo, while not an absolute \"never,\" deploying on a Friday is a gamble that usually isn\\'t worth the potential consequences. It\\'s a sign of good DevOps practice to be mindful of the impact of your deployments on your team and your users.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019ba60e-575b-71c0-8012-528aedb04052-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 13, 'output_tokens': 899, 'total_tokens': 912, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Static prompt (fixed prompts)\n",
    "prompt = [\n",
    "    (\"system\", \"You are a DevOps Engineer\"),   ### type , \"system\", \"user\", \"ai\"\n",
    "    (\"user\", \"Why not to deploy on friday ?\")\n",
    "]\n",
    "res = llm.invoke(prompt)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bcf3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, the age-old question! Deploying on a Friday is a classic DevOps taboo, and for good reason/n While there are always exceptions and sometimes you might have to, the general wisdom is to **avoid deploying on Fridays, especially late on a Friday/n**\n",
      "\n",
      "Here's a breakdown of why it's generally a bad idea:\n",
      "\n",
      "**1/n The Weekend is Your Enemy (for Rollbacks):**\n",
      "\n",
      "*   **Limited Support Staff:** Most of your critical engineering, operations, and support teams are likely winding down for the weekend/n If something goes wrong, getting the right people on a call, making quick decisions, and executing a rollback can be significantly harder and slower/n\n",
      "*   **Reduced Availability:** Key personnel might be out of town, on vacation, or simply unreachable, making incident response a nightmare/n\n",
      "*   **Escalation Challenges:** If a critical issue arises, escalating it to the right people on a Saturday or Sunday can be a real struggle, leading to prolonged downtime and frustrated users/n\n",
      "\n",
      "**2/n Increased Risk of Unforeseen Issues:**\n",
      "\n",
      "*   **\"It Worked on My Machine\" Syndrome:** Friday deployments are often rushed/n Developers might be eager to finish up for the week, leading to less thorough testing or overlooking edge cases/n\n",
      "*   **Intermittent Bugs:** Some bugs only manifest under specific load conditions or at certain times/n A Friday deployment might seem fine initially, but problems could surface over the weekend when fewer people are monitoring/n\n",
      "*   **External Dependencies:** If your deployment relies on external services or third-party APIs, their behavior over the weekend might be different or they might have their own maintenance windows, leading to unexpected failures/n\n",
      "\n",
      "**3/n Impact on User Experience:**\n",
      "\n",
      "*   **Downtime is More Painful:** If your application experiences downtime or critical bugs over the weekend, a larger percentage of your user base might be affected/n People have more free time to use your services, and a broken experience can lead to significant dissatisfaction/n\n",
      "*   **Reputational Damage:** A major outage over a weekend can be highly visible and negatively impact your company's reputation/n\n",
      "\n",
      "**4/n Reduced Morale and Work-Life Balance:**\n",
      "\n",
      "*   **The Dreaded Weekend Call:** Nobody wants to spend their weekend firefighting a production issue caused by a deployment/n This can lead to burnout and resentment among the team/n\n",
      "*   **Unplanned Work:** A failed Friday deployment often means the entire team (or at least the on-call folks) is stuck working through the weekend, ruining their plans/n\n",
      "\n",
      "**5/n Difficulty in Getting Feedback and Iteration:**\n",
      "\n",
      "*   **Slower Feedback Loop:** If you need to gather feedback on the new release or make quick adjustments, the weekend significantly slows down this process/n\n",
      "*   **Missed Opportunities:** You might miss out on valuable user feedback or the opportunity to iterate on a new feature quickly because the team is unavailable/n\n",
      "\n",
      "**When might you *consider* a Friday deploy (with extreme caution)?**\n",
      "\n",
      "*   **Trivial, Low-Risk Updates:** If it's a very minor bug fix, a small configuration change, or a feature with a very limited blast radius and a robust rollback plan/n\n",
      "*   **Urgent Security Patches:** In critical situations where a vulnerability needs immediate addressing, a Friday deploy might be unavoidable, but it should be accompanied by maximum vigilance and preparedness/n\n",
      "*   **Pre-Planned Maintenance with Communication:** If you have a scheduled maintenance window that happens to fall on a Friday, and you've communicated this extensively to your users, it might be acceptable/n\n",
      "*   **Highly Automated and Resilient Systems:** If your deployment pipeline is incredibly robust, your monitoring is top-notch, and your rollback strategy is nearly instantaneous, the risk might be lower/n\n",
      "\n",
      "**The Golden Rule:**\n",
      "\n",
      "The best practice is to deploy earlier in the week (Monday, Tuesday, or Wednesday) to give yourself ample time to:\n",
      "\n",
      "*   Monitor the deployment/n\n",
      "*   Address any issues that arise/n\n",
      "*   Gather feedback/n\n",
      "*   Perform rollbacks if necessary/n\n",
      "*   Enjoy your weekend!\n",
      "\n",
      "So, while not an absolute \"never,\" deploying on a Friday is a gamble that usually isn't worth the potential consequences/n It's a sign of good DevOps practice to be mindful of the impact of your deployments on your team and your users/n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(res.content.replace(\".\", \"/n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74eeac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
